{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda08cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gerekli kütüphaneleri indiriyoruz\n",
    "%pip install pandas scikit-learn\n",
    "%pip install matplotlib\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605f1926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test ve train setlerimizi import ediyoruz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train = pd.read_csv(\"evler-train.csv\")\n",
    "test = pd.read_csv(\"evler-test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a978f",
   "metadata": {},
   "source": [
    "Verilerin yapısı \"evNo,fiyat,brutM2,netM2,oda,salon,yas,kat,katMaks,dogalGaz,banyo,amerikan,balkon,asansor,otopark,esya,siteMi,aidat,guneyMi\" şeklinde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb81c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#veri ön işleme ile değeri -1 (yani boş) olan verileri, knn kullanarak dolduruyoruz\n",
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "\n",
    "# Convert -1 to NaN so KNN Imputer can recognize them as missing values\n",
    "train = train.replace(-1, np.nan)\n",
    "test = test.replace(-1, np.nan)\n",
    "\n",
    "# Now apply KNN imputation\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "train = pd.DataFrame(imputer.fit_transform(train), columns=train.columns)\n",
    "test = pd.DataFrame(imputer.transform(test), columns=test.columns)\n",
    "\n",
    "print(\"Missing values after imputation:\")\n",
    "print(f\"Train: {train.isna().sum().sum()}\")\n",
    "print(f\"Test: {test.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cef395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling - critical for SVR and can improve other models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate features and target\n",
    "X_train = train.drop(columns=[\"fiyat\", \"evNo\"])\n",
    "y_train = train[\"fiyat\"]\n",
    "X_test = test.drop(columns=[\"fiyat\", \"evNo\"])\n",
    "y_test = test[\"fiyat\"]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "print(\"Feature scaling completed\")\n",
    "print(f\"Train shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed35a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 farklı regresyon modeli deniyoruz\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3e145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lineer regresyon modeli\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(train.drop(columns=[\"fiyat\", \"evNo\"]), train[\"fiyat\"])\n",
    "lr_predictions = lr_model.predict(test.drop(columns=[\"fiyat\", \"evNo\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ce96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DecisionTreeRegressor modeli\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "dt_model.fit(train.drop(columns=[\"fiyat\", \"evNo\"]), train[\"fiyat\"])\n",
    "dt_predictions = dt_model.predict(test.drop(columns=[\"fiyat\", \"evNo\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7482823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForestRegressor modeli\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(train.drop(columns=[\"fiyat\", \"evNo\"]), train[\"fiyat\"])\n",
    "rf_predictions = rf_model.predict(test.drop(columns=[\"fiyat\", \"evNo\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dec2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVR modeli\n",
    "svr_model = SVR()\n",
    "svr_model.fit(train.drop(columns=[\"fiyat\", \"evNo\"]), train[\"fiyat\"])\n",
    "svr_predictions = svr_model.predict(test.drop(columns=[\"fiyat\", \"evNo\"]))\n",
    "\n",
    "# Train seti tahminlerini alıyoruz\n",
    "lr_train_predictions = lr_model.predict(train.drop(columns=[\"fiyat\", \"evNo\"]))\n",
    "dt_train_predictions = dt_model.predict(train.drop(columns=[\"fiyat\", \"evNo\"]))\n",
    "rf_train_predictions = rf_model.predict(train.drop(columns=[\"fiyat\", \"evNo\"]))\n",
    "svr_train_predictions = svr_model.predict(train.drop(columns=[\"fiyat\", \"evNo\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dbcddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 farklı metrik ile modellerimizi değerlendiriyoruz (hem train hem test)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error\n",
    "\n",
    "def evaluate_model(true_values, predictions):\n",
    "    mae = mean_absolute_error(true_values, predictions)\n",
    "    mse = mean_squared_error(true_values, predictions)\n",
    "    r2 = r2_score(true_values, predictions)\n",
    "    medae = median_absolute_error(true_values, predictions)\n",
    "    return {\"MAE\": mae, \"MSE\": mse, \"R2\": r2, \"MedAE\": medae}\n",
    "\n",
    "# Test set evaluation\n",
    "lr_test_results = evaluate_model(test[\"fiyat\"], lr_predictions)\n",
    "dt_test_results = evaluate_model(test[\"fiyat\"], dt_predictions)\n",
    "rf_test_results = evaluate_model(test[\"fiyat\"], rf_predictions)\n",
    "svr_test_results = evaluate_model(test[\"fiyat\"], svr_predictions)\n",
    "\n",
    "# Train set evaluation\n",
    "lr_train_results = evaluate_model(train[\"fiyat\"], lr_train_predictions)\n",
    "dt_train_results = evaluate_model(train[\"fiyat\"], dt_train_predictions)\n",
    "rf_train_results = evaluate_model(train[\"fiyat\"], rf_train_predictions)\n",
    "svr_train_results = evaluate_model(train[\"fiyat\"], svr_train_predictions)\n",
    "\n",
    "# Test sonuçlarını tablo halinde gösteriyoruz\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "test_results_df = pd.DataFrame({\n",
    "    \"Model\": [\"Linear Regression\", \"Decision Tree\", \"Random Forest\", \"SVR\"],\n",
    "    \"MAE\": [lr_test_results[\"MAE\"], dt_test_results[\"MAE\"], rf_test_results[\"MAE\"], svr_test_results[\"MAE\"]],\n",
    "    \"MSE\": [lr_test_results[\"MSE\"], dt_test_results[\"MSE\"], rf_test_results[\"MSE\"], svr_test_results[\"MSE\"]],\n",
    "    \"R2\": [lr_test_results[\"R2\"], dt_test_results[\"R2\"], rf_test_results[\"R2\"], svr_test_results[\"R2\"]],\n",
    "    \"MedAE\": [lr_test_results[\"MedAE\"], dt_test_results[\"MedAE\"], rf_test_results[\"MedAE\"], svr_test_results[\"MedAE\"]]\n",
    "})\n",
    "\n",
    "display_test_df = test_results_df.copy()\n",
    "num_cols = [\"MAE\", \"MSE\", \"R2\", \"MedAE\"]\n",
    "display_test_df[num_cols] = display_test_df[num_cols].map(lambda x: f\"{x:,.2f}\")\n",
    "print(display_test_df)\n",
    "\n",
    "# Train sonuçlarını tablo halinde gösteriyoruz\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAIN SET RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "train_results_df = pd.DataFrame({\n",
    "    \"Model\": [\"Linear Regression\", \"Decision Tree\", \"Random Forest\", \"SVR\"],\n",
    "    \"MAE\": [lr_train_results[\"MAE\"], dt_train_results[\"MAE\"], rf_train_results[\"MAE\"], svr_train_results[\"MAE\"]],\n",
    "    \"MSE\": [lr_train_results[\"MSE\"], dt_train_results[\"MSE\"], rf_train_results[\"MSE\"], svr_train_results[\"MSE\"]],\n",
    "    \"R2\": [lr_train_results[\"R2\"], dt_train_results[\"R2\"], rf_train_results[\"R2\"], svr_train_results[\"R2\"]],\n",
    "    \"MedAE\": [lr_train_results[\"MedAE\"], dt_train_results[\"MedAE\"], rf_train_results[\"MedAE\"], svr_train_results[\"MedAE\"]]\n",
    "})\n",
    "\n",
    "display_train_df = train_results_df.copy()\n",
    "display_train_df[num_cols] = display_train_df[num_cols].map(lambda x: f\"{x:,.2f}\")\n",
    "print(display_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472e389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy'nin uzun çıktıları kesmemesi için\n",
    "np.set_printoptions(threshold=1000)\n",
    "\n",
    "# Her modelin tahminlerini gerçek değerlerle karşılaştıran grafik\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Gerçek değerler\n",
    "actual = test[\"fiyat\"].values\n",
    "x_pos = np.arange(len(actual))\n",
    "\n",
    "# Bar genişliği\n",
    "bar_width = 0.15\n",
    "\n",
    "# Her model için tahminleri çiz\n",
    "plt.bar(x_pos - 1.5*bar_width, actual, bar_width, label='Gerçek Değer', color='black', alpha=0.7)\n",
    "plt.bar(x_pos - 0.5*bar_width, lr_predictions, bar_width, label='Linear Regression', color='blue', alpha=0.7)\n",
    "plt.bar(x_pos + 0.5*bar_width, dt_predictions, bar_width, label='Decision Tree', color='green', alpha=0.7)\n",
    "plt.bar(x_pos + 1.5*bar_width, rf_predictions, bar_width, label='Random Forest', color='orange', alpha=0.7)\n",
    "plt.bar(x_pos + 2.5*bar_width, svr_predictions, bar_width, label='SVR', color='red', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Test Örnekleri')\n",
    "plt.ylabel('Fiyat')\n",
    "plt.title('Model Tahminlerinin Gerçek Değerlerle Karşılaştırılması')\n",
    "plt.xticks(x_pos, range(1, len(actual) + 1))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8979a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sonuçları karşılaştırarak en iyi modeli seçiyoruz (test seti bazında)\n",
    "results = {\n",
    "    \"Linear Regression\": lr_test_results,\n",
    "    \"Decision Tree\": dt_test_results,\n",
    "    \"Random Forest\": rf_test_results,\n",
    "    \"SVR\": svr_test_results\n",
    "}\n",
    "best_model = min(results, key=lambda x: results[x][\"MAE\"])\n",
    "print(\"En İyi Model:\", best_model, \"with Test MAE:\", results[best_model][\"MAE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5db112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#en iyi modelin; model.intercept_,std_err ve model.coef_ değerlerini yazdırıyoruz\n",
    "\n",
    "# formatlama fonksiyonları\n",
    "fmt_float = lambda x: f\"{x:,.2f}\"\n",
    "fmt_coef = lambda x: f\"{x:,.4f}\"\n",
    "fmt_array = lambda arr, fmt=fmt_coef: \", \".join(fmt(float(v)) for v in np.atleast_1d(arr))\n",
    "\n",
    "if best_model == \"Linear Regression\":\n",
    "    print(\"En İyi Model: Linear Regression\")\n",
    "    intercept = float(lr_model.intercept_)\n",
    "    coefs = lr_model.coef_\n",
    "    # get feature names from training dataframe (preserve column order)\n",
    "    feature_names = list(train.drop(columns=[\"fiyat\", \"evNo\"]).columns)\n",
    "    residuals = lr_model.predict(train.drop(columns=[\"fiyat\", \"evNo\"])) - train[\"fiyat\"]\n",
    "    std_err = float(np.std(residuals))\n",
    "    print(\"Intercept:\", fmt_float(intercept))\n",
    "    print(\"Coefficients:\")\n",
    "    # print coefficient with sequence number and feature name for clarity (1-based index)\n",
    "    for idx, (name, c) in enumerate(zip(feature_names, coefs), start=1):\n",
    "        print(f\"  {idx}. {name}: {fmt_coef(c)}\")\n",
    "    print(\"Standard Error (train residuals):\", fmt_float(std_err))\n",
    "\n",
    "elif best_model == \"Decision Tree\":\n",
    "    print(\"En İyi Model: Decision Tree\")\n",
    "    residuals = dt_model.predict(train.drop(columns=[\"fiyat\", \"evNo\"])) - train[\"fiyat\"]\n",
    "    std_err = float(np.std(residuals))\n",
    "    print(\"Intercept: N/A for Decision Tree\")\n",
    "    print(\"Coefficients: N/A for Decision Tree\")\n",
    "    print(\"Standard Error (train residuals):\", fmt_float(std_err))\n",
    "\n",
    "elif best_model == \"Random Forest\":\n",
    "    print(\"En İyi Model: Random Forest\")\n",
    "    residuals = rf_model.predict(train.drop(columns=[\"fiyat\", \"evNo\"])) - train[\"fiyat\"]\n",
    "    std_err = float(np.std(residuals))\n",
    "    print(\"Intercept: N/A for Random Forest\")\n",
    "    print(\"Coefficients: N/A for Random Forest\")\n",
    "    print(\"Standard Error (train residuals):\", fmt_float(std_err))\n",
    "\n",
    "elif best_model == \"SVR\":\n",
    "    print(\"En İyi Model: SVR\")\n",
    "    # SVR intercept_ ve support_vectors_ olabilir\n",
    "    if hasattr(svr_model, \"intercept_\"):\n",
    "        intercepts = np.atleast_1d(svr_model.intercept_)\n",
    "        print(\"Intercept(s):\", fmt_array(intercepts, fmt=fmt_float))\n",
    "    else:\n",
    "        print(\"Intercept: N/A for SVR\")\n",
    "    if hasattr(svr_model, \"support_vectors_\"):\n",
    "        sv = svr_model.support_vectors_\n",
    "        print(\"Support vectors count:\", sv.shape[0])\n",
    "        # İlk 5 support vector'u göster\n",
    "        n_show = min(5, sv.shape[0])\n",
    "        print(f\"First {n_show} support vectors:\")\n",
    "        for i in range(n_show):\n",
    "            print(\" \", fmt_array(sv[i], fmt=fmt_coef))\n",
    "    else:\n",
    "        print(\"Support Vectors: N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806cd0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_intercept_, std_err ve model.coef_ değerlerini grafik olarak gösteriyoruz\n",
    "if best_model == \"Linear Regression\":\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    coef_vals = lr_model.coef_\n",
    "    feature_names = list(train.drop(columns=[\"fiyat\", \"evNo\"]).columns)\n",
    "    # build numbered labels like '1. brutM2', '2. netM2', ... for the x-ticks\n",
    "    numbered_labels = [f\"{i+1}. {n}\" for i, n in enumerate(feature_names)]\n",
    "    plt.bar(range(len(coef_vals)), coef_vals)\n",
    "    plt.title(\"Linear Regression Coefficients\")\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"Coefficient Value\")\n",
    "    # label x-ticks with feature names and rotate for readability\n",
    "    plt.xticks(range(len(feature_names)), numbered_labels, rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Coefficient plot is only available for Linear Regression model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c71125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance grafikleri (tree-based modeller için)\n",
    "feature_names = list(train.drop(columns=[\"fiyat\", \"evNo\"]).columns)\n",
    "numbered_labels = [f\"{i+1}. {n}\" for i, n in enumerate(feature_names)]\n",
    "\n",
    "if best_model == \"Decision Tree\":\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    importances = dt_model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(range(len(importances)), importances[indices])\n",
    "    plt.title(\"Decision Tree - Feature Importance (Sorted)\")\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"Importance\")\n",
    "    plt.xticks(range(len(feature_names)), [numbered_labels[i] for i in indices], rotation=45, ha=\"right\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(range(len(importances)), importances[indices])\n",
    "    plt.title(\"Decision Tree - Feature Importance (Horizontal)\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.yticks(range(len(feature_names)), [numbered_labels[i] for i in indices])\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top 5 features\n",
    "    print(\"\\nTop 5 Most Important Features:\")\n",
    "    for i in range(min(5, len(importances))):\n",
    "        idx = indices[i]\n",
    "        print(f\"{i+1}. {feature_names[idx]}: {importances[idx]:.4f}\")\n",
    "\n",
    "elif best_model == \"Random Forest\":\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    importances = rf_model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(range(len(importances)), importances[indices])\n",
    "    plt.title(\"Random Forest - Feature Importance (Sorted)\")\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"Importance\")\n",
    "    plt.xticks(range(len(feature_names)), [numbered_labels[i] for i in indices], rotation=45, ha=\"right\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(range(len(importances)), importances[indices])\n",
    "    plt.title(\"Random Forest - Feature Importance (Horizontal)\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.yticks(range(len(feature_names)), [numbered_labels[i] for i in indices])\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top 5 features\n",
    "    print(\"\\nTop 5 Most Important Features:\")\n",
    "    for i in range(min(5, len(importances))):\n",
    "        idx = indices[i]\n",
    "        print(f\"{i+1}. {feature_names[idx]}: {importances[idx]:.4f}\")\n",
    "\n",
    "elif best_model == \"Linear Regression\":\n",
    "    # Coefficient visualization already handled in previous cell\n",
    "    print(\"\\nFeature coefficients already displayed in the previous cell.\")\n",
    "    \n",
    "elif best_model == \"SVR\":\n",
    "    print(\"\\nSVR does not provide direct feature importance.\")\n",
    "    print(\"Consider using permutation importance or SHAP values for interpretation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
